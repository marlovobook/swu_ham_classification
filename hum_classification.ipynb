{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HAM10000 EfficientNetB0 + Metadata (Thomas et al., 2021 inspired)\n",
    "- 3-channel RGB input to use ImageNet weights (fixes shape mismatch)\n",
    "- Stratified 80/10/10 split\n",
    "- On-the-fly augmentation: rotation, translation, zoom, H/V flips\n",
    "- Oversampling (replication) on training set via sample_from_datasets\n",
    "- Metadata branch (age scaled, sex one-hot, localization one-hot)\n",
    "- EfficientNetB0 image branch, concatenate + MLP head\n",
    "- Works on VRAM ~16GB with reasonable batch sizes\n",
    "\n",
    "Adjust the CONFIG block for your paths.\n",
    "Tested with TensorFlow 2.15+ / Keras 3+, but compatible with 2.10+ with minor tweaks.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    \"IMG_DIRS\": [\n",
    "        r\"C:\\Aul\\com_vis\\HAM10000_images_part_1\",\n",
    "        r\"C:\\Aul\\com_vis\\HAM10000_images_part_2\",\n",
    "    ],\n",
    "    \"METADATA_CSV\": r\"C:\\Aul\\com_vis\\HAM10000_metadata.csv\",  # columns include: image_id, dx, age, sex, localization\n",
    "    \"IMG_SIZE\": 224,\n",
    "    \"BATCH\": 32,                 # Use 32–64 if VRAM allows\n",
    "    \"EPOCHS\": 50,\n",
    "    \"BACKBONE\": \"EfficientNetB0\", # Keep B0 for VRAM 16GB\n",
    "    \"MIXED_PRECISION\": True,     # Set False if you hit numeric issues\n",
    "    \"OUTPUT_DIR\": r\"C:/Aul/com_vis/outputs_ham10000\",\n",
    "    \"SEED\": 42,\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"OUTPUT_DIR\"], exist_ok=True)\n",
    "\n",
    "# Silence TF INFO logs (optional)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Mixed precision (recommended on modern GPUs)\n",
    "if CONFIG[\"MIXED_PRECISION\"]:\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Mixed precision not available:\", e)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# LABELS\n",
    "# -------------------------------------------------------------\n",
    "CLASSES = [\"akiec\", \"bcc\", \"bkl\", \"df\", \"mel\", \"nv\", \"vasc\"]\n",
    "CLASS2IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# LOAD METADATA & BUILD DATAFRAME\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def collect_image_paths(img_dirs):\n",
    "    paths = []\n",
    "    for d in img_dirs:\n",
    "        if os.path.isdir(d):\n",
    "            for fn in os.listdir(d):\n",
    "                if fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    paths.append(os.path.join(d, fn))\n",
    "    return paths\n",
    "\n",
    "all_paths = collect_image_paths(CONFIG[\"IMG_DIRS\"])\n",
    "print(f\"Found {len(all_paths)} image files from provided folders.\")\n",
    "\n",
    "# Load metadata\n",
    "md = pd.read_csv(CONFIG[\"METADATA_CSV\"])  # expects columns: image_id, dx(label), age, sex, localization\n",
    "\n",
    "# map image_id -> full path\n",
    "# image files names are like ISIC_0024306.jpg ; metadata has image_id without extension\n",
    "name2path: Dict[str, str] = {}\n",
    "for p in all_paths:\n",
    "    base = os.path.basename(p)\n",
    "    stem = os.path.splitext(base)[0]\n",
    "    name2path[stem] = p\n",
    "\n",
    "# Build dataframe by joining metadata with resolved paths\n",
    "# Filter to our 7 dx classes\n",
    "md = md[md[\"dx\"].isin(CLASSES)].copy()\n",
    "md[\"label_idx\"] = md[\"dx\"].map(CLASS2IDX).astype(int)\n",
    "\n",
    "# Fix pandas chained assignment: assign back safely\n",
    "if md[\"age\"].isna().all():\n",
    "    # Rare case, fallback to 45\n",
    "    md[\"age\"] = 45\n",
    "else:\n",
    "    md[\"age\"] = md[\"age\"].fillna(md[\"age\"].median())\n",
    "\n",
    "# Normalize categorical entries\n",
    "md[\"sex\"] = md[\"sex\"].fillna(\"unknown\").str.lower()\n",
    "md[\"localization\"] = md[\"localization\"].fillna(\"unknown\").str.lower()\n",
    "\n",
    "# Resolve paths; drop rows with missing files\n",
    "md[\"image_path\"] = md[\"image_id\"].map(name2path)\n",
    "md = md[~md[\"image_path\"].isna()].copy()\n",
    "\n",
    "print(\"Classes:\", CLASSES)\n",
    "print(\"Total images:\", len(md))\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STRATIFIED SPLIT 80/10/10\n",
    "# -------------------------------------------------------------\n",
    "train_df, temp_df = train_test_split(\n",
    "    md, test_size=0.2, stratify=md[\"label_idx\"], random_state=CONFIG[\"SEED\"]\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"label_idx\"], random_state=CONFIG[\"SEED\"]\n",
    ")\n",
    "print(f\"Split -> train:{len(train_df)}  val:{len(val_df)}  test:{len(test_df)}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# METADATA ENCODING (age scaling, one-hots)\n",
    "# -------------------------------------------------------------\n",
    "# Fit encoders on *all* data to keep consistent dimensions\n",
    "sex_values = sorted(md[\"sex\"].unique().tolist())\n",
    "loc_values = sorted(md[\"localization\"].unique().tolist())\n",
    "SEX2IDX = {s: i for i, s in enumerate(sex_values)}\n",
    "LOC2IDX = {l: i for i, l in enumerate(loc_values)}\n",
    "\n",
    "age_scaler = StandardScaler()\n",
    "age_scaler.fit(md[[\"age\"]].values.astype(np.float32))\n",
    "\n",
    "meta_dim = 1 + len(SEX2IDX) + len(LOC2IDX)  # age + sex one-hot + loc one-hot\n",
    "\n",
    "# Helper to convert one row -> meta vector\n",
    "\n",
    "def row_to_meta(row) -> np.ndarray:\n",
    "    age = row[\"age\"]\n",
    "    sex = row[\"sex\"]\n",
    "    loc = row[\"localization\"]\n",
    "\n",
    "    age_scaled = age_scaler.transform([[float(age)]])[0, 0]\n",
    "\n",
    "    sex_oh = np.zeros(len(SEX2IDX), dtype=np.float32)\n",
    "    sex_oh[SEX2IDX.get(sex, 0)] = 1.0\n",
    "\n",
    "    loc_oh = np.zeros(len(LOC2IDX), dtype=np.float32)\n",
    "    loc_oh[LOC2IDX.get(loc, 0)] = 1.0\n",
    "\n",
    "    return np.concatenate([[age_scaled], sex_oh, loc_oh]).astype(np.float32)\n",
    "\n",
    "# Build arrays for tf.data\n",
    "\n",
    "def build_np_arrays(df):\n",
    "    paths = df[\"image_path\"].tolist()\n",
    "    labels = df[\"label_idx\"].astype(np.int32).values\n",
    "    metas = np.vstack([row_to_meta(r) for _, r in df.iterrows()])\n",
    "    return np.array(paths), metas, labels\n",
    "\n",
    "train_paths, train_meta, train_y = build_np_arrays(train_df)\n",
    "val_paths,   val_meta,   val_y   = build_np_arrays(val_df)\n",
    "test_paths,  test_meta,  test_y  = build_np_arrays(test_df)\n",
    "\n",
    "# Save label mapping for later inference\n",
    "with open(os.path.join(CONFIG[\"OUTPUT_DIR\"], \"class_indices.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({i: c for c, i in CLASS2IDX.items()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# TF.DATA PIPELINES\n",
    "# -------------------------------------------------------------\n",
    "IMG_SIZE = CONFIG[\"IMG_SIZE\"]\n",
    "\n",
    "@tf.function\n",
    "def preprocess_image(path):\n",
    "    img_bytes = tf.io.read_file(path)\n",
    "    # Force decode to 3 channels to match ImageNet weights\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), method=tf.image.ResizeMethod.BILINEAR)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # EfficientNet expects [0,255] then preprocess_input; if you prefer, use rescaling(1/255.) + normalization below\n",
    "    return img\n",
    "\n",
    "# Augment\n",
    "augment = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.1),                 # ~±10°\n",
    "    layers.RandomTranslation(0.1, 0.1),        # 10% shift\n",
    "    layers.RandomZoom((-0.1, 0.1)),            # zoom in/out\n",
    "], name=\"augment\")\n",
    "\n",
    "# EfficientNetB0 preprocess\n",
    "try:\n",
    "    from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n",
    "except Exception:\n",
    "    # Fallback: simple rescale to [0,1]\n",
    "    effnet_preprocess = lambda x: x / 255.0\n",
    "\n",
    "\n",
    "def make_dataset(paths, metas, labels, training: bool, batch_size: int):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, metas, labels))\n",
    "\n",
    "    def _map(path, meta, y):\n",
    "        img = preprocess_image(path)\n",
    "        if training:\n",
    "            img = augment(img, training=True)\n",
    "        # Apply preprocess (expects float32)\n",
    "        img = effnet_preprocess(img)\n",
    "        return {\"image\": img, \"meta\": meta}, y\n",
    "\n",
    "    if training:\n",
    "        ds = ds.shuffle(4096, seed=CONFIG[\"SEED\"], reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Per-class datasets for oversampling (replication)\n",
    "\n",
    "def make_per_class_datasets(paths, metas, labels, batch):\n",
    "    per_class = []\n",
    "    paths = np.array(paths)\n",
    "    metas = np.array(metas)\n",
    "    labels = np.array(labels)\n",
    "    for c in range(len(CLASSES)):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        ds_c = tf.data.Dataset.from_tensor_slices((paths[idx], metas[idx], labels[idx]))\n",
    "        ds_c = ds_c.shuffle(len(idx), seed=CONFIG[\"SEED\"], reshuffle_each_iteration=True)\n",
    "        ds_c = ds_c.repeat()  # allow infinite sampling\n",
    "        per_class.append(ds_c)\n",
    "    # Equal weights for each class\n",
    "    weights = [1.0 / len(CLASSES)] * len(CLASSES)\n",
    "\n",
    "    def _map(path, meta, y):\n",
    "        img = preprocess_image(path)\n",
    "        img = augment(img, training=True)\n",
    "        img = effnet_preprocess(img)\n",
    "        return {\"image\": img, \"meta\": meta}, y\n",
    "\n",
    "    balanced = tf.data.Dataset.sample_from_datasets(per_class, weights=weights, seed=CONFIG[\"SEED\"])\n",
    "    balanced = balanced.map(_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    balanced = balanced.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return balanced\n",
    "\n",
    "# Build datasets\n",
    "train_ds_balanced = make_per_class_datasets(train_paths, train_meta, train_y, CONFIG[\"BATCH\"])  # Oversampled\n",
    "val_ds  = make_dataset(val_paths, val_meta, val_y, training=False, batch_size=CONFIG[\"BATCH\"]) \n",
    "test_ds = make_dataset(test_paths, test_meta, test_y, training=False, batch_size=CONFIG[\"BATCH\"]) \n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# MODEL\n",
    "# -------------------------------------------------------------\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Inputs\n",
    "img_in = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"image\")\n",
    "meta_in = layers.Input(shape=(meta_dim,), name=\"meta\")\n",
    "\n",
    "# Backbone (ImageNet weights) + global pooling\n",
    "# NOTE: include_top=False and pooling='avg' give a compact feature vector\n",
    "base = EfficientNetB0(include_top=False, weights=None, input_tensor=img_in, pooling=\"avg\")\n",
    "img_feat = layers.Dropout(0.2, name=\"img_dropout\")(base.output)\n",
    "\n",
    "# Metadata branch\n",
    "m = layers.Dense(64, activation=\"relu\")(meta_in)\n",
    "m = layers.Dropout(0.2)(m)\n",
    "m = layers.Dense(32, activation=\"relu\")(m)\n",
    "\n",
    "# Fuse branches\n",
    "h = layers.Concatenate(name=\"fuse\")([img_feat, m])\n",
    "h = layers.Dense(128, activation=\"relu\")(h)\n",
    "h = layers.Dropout(0.3)(h)\n",
    "\n",
    "# IMPORTANT: in mixed precision, keep final dtype float32\n",
    "out = layers.Dense(len(CLASSES), activation=\"softmax\", dtype=\"float32\", name=\"pred\")(h)\n",
    "\n",
    "model = Model(inputs=[img_in, meta_in], outputs=out)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# TRAINING\n",
    "# -------------------------------------------------------------\n",
    "steps_per_epoch = math.ceil(len(train_df) / CONFIG[\"BATCH\"])  # with oversampling .repeat(), we need steps\n",
    "\n",
    "ckpt_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"best_efficientnetb0_meta.keras\")\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=8, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "    ReduceLROnPlateau(patience=4, factor=0.2, monitor=\"val_loss\", min_lr=1e-6),\n",
    "    ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds_balanced,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG[\"EPOCHS\"],\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "hist_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"train_history.json\")\n",
    "with open(hist_path, \"w\") as f:\n",
    "    json.dump({k: [float(x) for x in v] for k, v in history.history.items()}, f, indent=2)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# EVALUATION\n",
    "# -------------------------------------------------------------\n",
    "print(\"Evaluating on test set …\")\n",
    "results = model.evaluate(test_ds, verbose=1)\n",
    "print(dict(zip(model.metrics_names, results)))\n",
    "\n",
    "# Save model\n",
    "final_path = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"final_efficientnetb0_meta.keras\")\n",
    "model.save(final_path)\n",
    "print(\"Saved model to:\", final_path)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# NOTES\n",
    "# -------------------------------------------------------------\n",
    "# - If you still see oneDNN logs, it's just informational. To disable oneDNN completely:\n",
    "#   os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"  (set BEFORE importing tensorflow)\n",
    "# - If you prefer class_weight instead of replication oversampling:\n",
    "#   from sklearn.utils.class_weight import compute_class_weight\n",
    "#   cw = compute_class_weight('balanced', classes=np.arange(len(CLASSES)), y=train_y)\n",
    "#   class_weight = {i: float(w) for i, w in enumerate(cw)}\n",
    "#   model.fit(train_ds, validation_data=val_ds, class_weight=class_weight, ...)\n",
    "# - To switch to EfficientNetB3, just import EfficientNetB3 and keep 3-channel input. Increase IMG_SIZE (e.g., 300) and reduce BATCH if OOM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41eb9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
